{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Standardization Pipeline\n",
    "\n",
    "This notebook is dedicated to the preprocessing and standardization of taxi trip data stored in Parquet format. The steps performed in this notebook include:\n",
    "\n",
    "1. **Initialization**: Setting up a Spark session for efficient data processing and configuring session parameters.\n",
    "2. **Loading and Inspecting Data**: Reading multiple Parquet files from a specified directory and inspecting their schemas to understand the structure and types of the data columns.\n",
    "3. **Schema Standardization**: Defining and applying a function to standardize the schema across all datasets, ensuring consistent data types and column names for seamless merging.\n",
    "4. **Combining DataFrames**: Merging all standardized datasets into a single DataFrame, which is essential for subsequent data analysis or machine learning tasks.\n",
    "5. **Saving the Processed Data**: Writing the combined DataFrame back to disk in Parquet format, making it available for further analysis in subsequent stages of the project.\n",
    "6. **Session Cleanup**: Stopping the Spark session to release resources and conclude the data preprocessing pipeline.\n",
    "\n",
    "This notebook ensures that all datasets are in a consistent format and structure, paving the way for accurate and efficient data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries\n",
    "- `os`: Used for interacting with the operating system to handle file paths.\n",
    "- `pyspark.sql.SparkSession`: The entry point to programming Spark with the Dataset and DataFrame API.\n",
    "- `pyspark.sql.functions.col`: A function to access columns within DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Spark Session\n",
    "Initialize a Spark session with specific configurations:\n",
    "- **appName**: The name of our Spark application, useful for identifying the application in logs and UIs.\n",
    "- **spark.sql.repl.eagerEval.enabled**: Enables eager evaluation for better performance in interactive environments.\n",
    "- **spark.sql.parquet.cacheMetadata**: Caches Parquet metadata for faster access.\n",
    "- **spark.sql.session.timeZone**: Sets the session's time zone to UTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/23 17:01:54 WARN Utils: Your hostname, apples-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.13.11.182 instead (on interface en0)\n",
      "24/08/23 17:01:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/23 17:01:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Displaying Schema of Parquet Files\n",
    "The following code reads all `.parquet` files in the specified directory, prints the schema for each file, and displays the path of the file being processed.\n",
    "\n",
    "- **data_path**: The directory path where the Parquet files are stored.\n",
    "- **parquet_files**: A list of paths to all Parquet files in the `data_path` directory.\n",
    "- **df.printSchema()**: Displays the schema (i.e., structure) of each DataFrame loaded from a Parquet file, helping us understand the data types and structure of our datasets.\n",
    "\n",
    "The output below shows the schema of the Parquet files, providing an understanding of the structure and types of the data columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for file: /Users/jennymai/Desktop/data_sci/mast_project1/data/landing/yellow_2022_12.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Schema for file: /Users/jennymai/Desktop/data_sci/mast_project1/data/landing/yellow_2023_04.parquet\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Schema for file: /Users/jennymai/Desktop/data_sci/mast_project1/data/landing/yellow_2022_11.parquet\n",
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Schema for file: /Users/jennymai/Desktop/data_sci/mast_project1/data/landing/yellow_2023_03.parquet\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Schema for file: /Users/jennymai/Desktop/data_sci/mast_project1/data/landing/yellow_2023_02.parquet\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Schema for file: /Users/jennymai/Desktop/data_sci/mast_project1/data/landing/yellow_2023_01.parquet\n",
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data_path = '/Users/jennymai/Desktop/data_sci/mast_project1/data/landing'\n",
    "\n",
    "parquet_files = [os.path.join(data_path, f) for f in os.listdir(data_path) if f.endswith('.parquet')]\n",
    "\n",
    "for file in parquet_files:\n",
    "    print(f\"Schema for file: {file}\")\n",
    "    df = spark.read.parquet(file)\n",
    "    df.printSchema()\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing Schema Across Parquet Files\n",
    "This section defines a function to standardize the schema of the Parquet files. We will apply this function to each DataFrame and then combine all the DataFrames into a single DataFrame with a consistent schema.\n",
    "\n",
    "- **standardize_schema function**: This function standardizes the schema by ensuring that specific columns have consistent data types across all DataFrames. This is important for when we later combine these DataFrames.\n",
    "- **Combining DataFrames**: We iterate over each Parquet file, apply the standardization, and then combine the resulting DataFrames into a single DataFrame (`sdf`) using `unionByName`, which ensures that the columns are aligned by name.\n",
    "- **Printing Final Schema**: Finally, we print the schema of the combined DataFrame to verify that all columns have consistent data types and names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def standardize_schema(df):\n",
    "    \"\"\"\n",
    "    Standardizes the schema of a DataFrame by:\n",
    "    - Casting specific columns to consistent data types.\n",
    "    - Renaming the 'Airport_fee' column to 'airport_fee' for consistency.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame to be standardized.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A new DataFrame with standardized schema.\n",
    "    \"\"\"\n",
    "    return (df\n",
    "            .withColumn(\"VendorID\", col(\"VendorID\").cast(\"int\"))  # Casting 'VendorID' to integer\n",
    "            .withColumn(\"passenger_count\", col(\"passenger_count\").cast(\"double\"))  # Casting 'passenger_count' to double\n",
    "            .withColumn(\"RatecodeID\", col(\"RatecodeID\").cast(\"double\"))  # Casting 'RatecodeID' to double\n",
    "            .withColumn(\"PULocationID\", col(\"PULocationID\").cast(\"int\"))  # Casting 'PULocationID' to integer\n",
    "            .withColumn(\"DOLocationID\", col(\"DOLocationID\").cast(\"int\"))  # Casting 'DOLocationID' to integer\n",
    "            .withColumnRenamed(\"Airport_fee\", \"airport_fee\")  # Renaming 'Airport_fee' to 'airport_fee'\n",
    "           )\n",
    "\n",
    "# Recreate the list of parquet files from the data path\n",
    "parquet_files = [os.path.join(data_path, f) for f in os.listdir(data_path) if f.endswith('.parquet')]\n",
    "\n",
    "sdf = None  # Initialize an empty DataFrame to accumulate the standardized DataFrames\n",
    "\n",
    "for file in parquet_files:\n",
    "    df = spark.read.parquet(file)  # Read each parquet file into a DataFrame\n",
    "    standardized_df = standardize_schema(df)  # Apply schema standardization\n",
    "    \n",
    "    if sdf is None:\n",
    "        sdf = standardized_df  # Set the first DataFrame as the initial value of sdf\n",
    "    else:\n",
    "        sdf = sdf.unionByName(standardized_df)  # Union the standardized DataFrame with the accumulating sdf\n",
    "\n",
    "sdf.printSchema()  # Print the schema of the final combined DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the Final DataFrame to Parquet and Stopping Spark Session\n",
    "In this final step, we write the combined and standardized DataFrame to a new Parquet file, and then we stop the Spark session to release resources.\n",
    "\n",
    "- **Writing the DataFrame**: The `sdf.write.parquet()` function writes the combined DataFrame (`sdf`) to the specified directory in Parquet format. The `mode='overwrite'` parameter ensures that any existing files in the target directory are replaced.\n",
    "- **Stopping the Spark Session**: The `spark.stop()` command stops the Spark session, which is important to free up resources and clean up the environment after the data processing is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write the combined standardized DataFrame to a new Parquet file\n",
    "sdf.write.parquet('/Users/jennymai/Desktop/data_sci/mast_project1/data/raw', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session to release resources\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
