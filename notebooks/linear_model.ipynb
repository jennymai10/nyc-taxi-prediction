{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Data: Linear Regression Modeling\n",
    "\n",
    "In this notebook, we perform linear regression modeling on the NYC taxi dataset. We start by building a standard linear regression model, followed by a Lasso regression model. Additionally, we conduct model diagnostics and implement feature selection using backward elimination.\n",
    "\n",
    "## Libraries and Initial Setup\n",
    "\n",
    "We begin by importing the necessary libraries, initializing a Spark session, and loading the preprocessed dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql import DataFrame\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import probplot\n",
    "from statsmodels.stats.stattools import durbin_watson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 20:21:28 WARN Utils: Your hostname, apples-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.13.11.182 instead (on interface en0)\n",
      "24/08/25 20:21:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/25 20:21:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)  # Enable eager evaluation for interactive querying\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")  # Cache metadata for parquet files\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")  # Set the timezone to UTC\n",
    "    .getOrCreate()  # Create or retrieve the existing Spark session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape(sdf: DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Returns the shape of a Spark DataFrame as a tuple (number of rows, number of columns).\n",
    "\n",
    "    :param sdf: Spark DataFrame\n",
    "    :return: String stating the shape of sdf\n",
    "    \"\"\"\n",
    "    num_rows = sdf.count()\n",
    "    num_columns = len(sdf.columns)\n",
    "    print(f\"Shape of the DataFrame: {num_rows} rows, {num_columns} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We load the preprocessed data from a Parquet file into a Spark DataFrame for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 4) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: 4393759 rows, 36 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed data from a Parquet file\n",
    "df_train = spark.read.parquet('/Users/jennymai/Desktop/data_sci/mast_project1/data/development')\n",
    "shape(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 20:21:53 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>VendorID</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>RatecodeID</th><th>store_and_fwd_flag</th><th>PULocationID</th><th>DOLocationID</th><th>payment_type</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th><th>congestion_surcharge</th><th>airport_fee</th><th>trip_duration_mins</th><th>pickup_hour</th><th>pickup_dayofweek</th><th>dropoff_hour</th><th>dropoff_dayofweek</th><th>days_since_2022_11_01</th><th>distance_time_interaction</th><th>is_airport_trip</th><th>is_tourist_trip</th><th>pickup_at_airport</th><th>dropoff_at_airport</th><th>pickup_at_tourist_attraction</th><th>dropoff_at_tourist_attraction</th><th>is_holiday_season</th><th>is_event_day</th><th>avg_temp</th><th>precipitation</th></tr>\n",
       "<tr><td>1</td><td>2022-11-01 15:09:19</td><td>2022-11-01 15:14:39</td><td>1.0</td><td>0.2623642721597987</td><td>1.0</td><td>0</td><td>161</td><td>230</td><td>1</td><td>5.0</td><td>2.5</td><td>0.5</td><td>9.999999950000001E-9</td><td>0.0</td><td>0.3</td><td>8.3</td><td>2.5</td><td>0.0</td><td>1.8458266920772781</td><td>15</td><td>3</td><td>15</td><td>3</td><td>0</td><td>4.5</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>4.182050142793878</td><td>0.19062036787311248</td></tr>\n",
       "<tr><td>1</td><td>2022-11-01 16:55:56</td><td>2022-11-01 17:06:46</td><td>2.0</td><td>0.7419373494912821</td><td>1.0</td><td>0</td><td>162</td><td>100</td><td>1</td><td>8.0</td><td>3.5</td><td>0.5</td><td>1.2383742339418191</td><td>0.0</td><td>0.3</td><td>14.75</td><td>2.5</td><td>0.0</td><td>2.4709204086583307</td><td>16</td><td>3</td><td>17</td><td>3</td><td>0</td><td>17.6</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>4.182050142793878</td><td>0.19062036787311248</td></tr>\n",
       "<tr><td>1</td><td>2022-11-01 18:03:50</td><td>2022-11-01 18:35:54</td><td>1.0</td><td>1.5260563056689624</td><td>1.0</td><td>0</td><td>263</td><td>48</td><td>1</td><td>20.0</td><td>3.5</td><td>0.5</td><td>1.163150812930681</td><td>0.0</td><td>0.3</td><td>26.5</td><td>2.5</td><td>0.0</td><td>3.4985257259251368</td><td>18</td><td>3</td><td>18</td><td>3</td><td>0</td><td>64.8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>4.182050142793878</td><td>0.19062036787311248</td></tr>\n",
       "<tr><td>1</td><td>2022-11-01 18:14:45</td><td>2022-11-01 18:34:13</td><td>2.0</td><td>0.993251776713987</td><td>1.0</td><td>0</td><td>237</td><td>230</td><td>1</td><td>13.0</td><td>3.5</td><td>0.5</td><td>0.009950340754158134</td><td>0.0</td><td>0.3</td><td>17.31</td><td>2.5</td><td>0.0</td><td>3.0187975469735866</td><td>18</td><td>3</td><td>18</td><td>3</td><td>0</td><td>30.599999999999998</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>4.182050142793878</td><td>0.19062036787311248</td></tr>\n",
       "<tr><td>1</td><td>2022-11-01 18:38:02</td><td>2022-11-01 19:00:26</td><td>1.0</td><td>1.3083328223528814</td><td>1.0</td><td>0</td><td>234</td><td>50</td><td>2</td><td>15.5</td><td>3.5</td><td>0.5</td><td>9.999999950000001E-9</td><td>0.0</td><td>0.3</td><td>19.8</td><td>2.5</td><td>0.0</td><td>3.1527360227910064</td><td>18</td><td>3</td><td>19</td><td>3</td><td>0</td><td>48.6</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>4.182050142793878</td><td>0.19062036787311248</td></tr>\n",
       "<tr><td>1</td><td>2022-11-01 19:28:00</td><td>2022-11-01 19:44:28</td><td>1.0</td><td>0.993251776713987</td><td>1.0</td><td>0</td><td>170</td><td>68</td><td>1</td><td>11.5</td><td>3.5</td><td>0.5</td><td>1.599387578600801</td><td>0.0</td><td>0.3</td><td>19.75</td><td>2.5</td><td>0.0</td><td>2.860294303231406</td><td>19</td><td>3</td><td>19</td><td>3</td><td>0</td><td>32.3</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>4.182050142793878</td><td>0.19062036787311248</td></tr>\n",
       "<tr><td>1</td><td>2022-11-01 23:41:47</td><td>2022-11-01 23:53:26</td><td>1.0</td><td>1.098612292001443</td><td>1.0</td><td>0</td><td>152</td><td>74</td><td>1</td><td>10.0</td><td>0.5</td><td>0.5</td><td>1.335001069363919</td><td>0.0</td><td>0.3</td><td>14.1</td><td>0.0</td><td>0.0</td><td>2.5376572159640434</td><td>23</td><td>3</td><td>23</td><td>3</td><td>0</td><td>46.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>4.182050142793878</td><td>0.19062036787311248</td></tr>\n",
       "<tr><td>1</td><td>2022-11-02 00:22:14</td><td>2022-11-02 00:37:21</td><td>1.0</td><td>1.131402114716907</td><td>1.0</td><td>0</td><td>230</td><td>158</td><td>2</td><td>9.0</td><td>3.0</td><td>0.5</td><td>9.999999950000001E-9</td><td>0.0</td><td>0.3</td><td>12.8</td><td>2.5</td><td>0.0</td><td>2.7798539338516695</td><td>0</td><td>4</td><td>0</td><td>4</td><td>1</td><td>0.0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>4.127134385206382</td><td>9.999999950000001E-9</td></tr>\n",
       "<tr><td>1</td><td>2022-11-02 07:08:00</td><td>2022-11-02 07:29:05</td><td>1.0</td><td>1.3083328223528814</td><td>1.0</td><td>0</td><td>107</td><td>140</td><td>1</td><td>14.5</td><td>2.5</td><td>0.5</td><td>0.6931471855599453</td><td>0.0</td><td>0.3</td><td>18.8</td><td>2.5</td><td>0.0</td><td>3.094823176651052</td><td>7</td><td>4</td><td>7</td><td>4</td><td>1</td><td>18.900000000000002</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>4.127134385206382</td><td>9.999999950000001E-9</td></tr>\n",
       "<tr><td>1</td><td>2022-11-02 08:13:38</td><td>2022-11-02 08:41:56</td><td>1.0</td><td>1.7578579192765116</td><td>1.0</td><td>0</td><td>163</td><td>261</td><td>1</td><td>20.0</td><td>2.5</td><td>0.5</td><td>1.3862943636198906</td><td>0.0</td><td>0.3</td><td>26.3</td><td>2.5</td><td>0.0</td><td>3.3775875163643185</td><td>8</td><td>4</td><td>8</td><td>4</td><td>1</td><td>38.4</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>4.127134385206382</td><td>9.999999950000001E-9</td></tr>\n",
       "<tr><td>1</td><td>2022-11-02 08:38:53</td><td>2022-11-02 08:57:15</td><td>1.0</td><td>0.7419373494912821</td><td>1.0</td><td>0</td><td>229</td><td>161</td><td>1</td><td>11.5</td><td>2.5</td><td>0.5</td><td>9.999999950000001E-9</td><td>0.0</td><td>0.3</td><td>14.8</td><td>2.5</td><td>0.0</td><td>2.963553375706107</td><td>8</td><td>4</td><td>8</td><td>4</td><td>1</td><td>8.8</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>4.127134385206382</td><td>9.999999950000001E-9</td></tr>\n",
       "<tr><td>1</td><td>2022-11-02 08:57:20</td><td>2022-11-02 09:15:21</td><td>1.0</td><td>1.335001069363919</td><td>1.0</td><td>0</td><td>238</td><td>48</td><td>1</td><td>13.5</td><td>2.5</td><td>0.5</td><td>1.4701758473994433</td><td>0.0</td><td>0.3</td><td>20.15</td><td>2.5</td><td>0.0</td><td>2.9453157881658294</td><td>8</td><td>4</td><td>9</td><td>4</td><td>1</td><td>22.4</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>4.127134385206382</td><td>9.999999950000001E-9</td></tr>\n",
       "<tr><td>1</td><td>2022-11-02 09:04:41</td><td>2022-11-02 09:16:56</td><td>1.0</td><td>0.7419373494912821</td><td>1.0</td><td>0</td><td>234</td><td>230</td><td>1</td><td>8.5</td><td>2.5</td><td>0.5</td><td>1.2119409769513032</td><td>0.0</td><td>0.3</td><td>14.16</td><td>2.5</td><td>0.0</td><td>2.5839975531869483</td><td>9</td><td>4</td><td>9</td><td>4</td><td>1</td><td>9.9</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>4.127134385206382</td><td>9.999999950000001E-9</td></tr>\n",
       "<tr><td>1</td><td>2022-11-02 09:45:19</td><td>2022-11-02 10:01:59</td><td>1.0</td><td>1.163150812930681</td><td>1.0</td><td>0</td><td>239</td><td>237</td><td>1</td><td>12.5</td><td>2.5</td><td>0.5</td><td>1.4231083366522455</td><td>0.0</td><td>0.3</td><td>18.95</td><td>2.5</td><td>0.0</td><td>2.87167962545005</td><td>9</td><td>4</td><td>10</td><td>4</td><td>1</td><td>19.8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>4.127134385206382</td><td>9.999999950000001E-9</td></tr>\n",
       "<tr><td>1</td><td>2022-11-02 11:07:32</td><td>2022-11-02 11:31:55</td><td>1.0</td><td>1.098612292001443</td><td>1.0</td><td>0</td><td>140</td><td>143</td><td>1</td><td>15.5</td><td>2.5</td><td>0.5</td><td>0.6931471855599453</td><td>0.0</td><td>0.3</td><td>19.8</td><td>2.5</td><td>0.0</td><td>3.2340927910670203</td><td>11</td><td>4</td><td>11</td><td>4</td><td>1</td><td>22.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>4.127134385206382</td><td>9.999999950000001E-9</td></tr>\n",
       "<tr><td>1</td><td>2022-11-02 12:19:31</td><td>2022-11-02 12:32:12</td><td>2.0</td><td>1.0296194207525868</td><td>1.0</td><td>0</td><td>140</td><td>229</td><td>3</td><td>10.0</td><td>2.5</td><td>0.5</td><td>9.999999950000001E-9</td><td>0.0</td><td>0.3</td><td>13.3</td><td>2.5</td><td>0.0</td><td>2.6161785479611437</td><td>12</td><td>4</td><td>12</td><td>4</td><td>1</td><td>21.6</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>4.127134385206382</td><td>9.999999950000001E-9</td></tr>\n",
       "<tr><td>1</td><td>2022-11-02 14:27:19</td><td>2022-11-02 14:51:20</td><td>2.0</td><td>1.2527629713525108</td><td>1.0</td><td>0</td><td>43</td><td>164</td><td>1</td><td>16.0</td><td>2.5</td><td>0.5</td><td>0.029558811950282222</td><td>0.0</td><td>0.3</td><td>19.33</td><td>2.5</td><td>0.0</td><td>3.219542269811095</td><td>14</td><td>4</td><td>14</td><td>4</td><td>1</td><td>35.0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>4.127134385206382</td><td>9.999999950000001E-9</td></tr>\n",
       "<tr><td>1</td><td>2022-11-02 15:31:47</td><td>2022-11-02 15:56:04</td><td>1.0</td><td>1.3083328223528814</td><td>1.0</td><td>0</td><td>100</td><td>140</td><td>1</td><td>16.0</td><td>2.5</td><td>0.5</td><td>1.098612292001443</td><td>0.0</td><td>0.3</td><td>21.3</td><td>2.5</td><td>0.0</td><td>3.2301454175219493</td><td>15</td><td>4</td><td>15</td><td>4</td><td>1</td><td>40.5</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>4.127134385206382</td><td>9.999999950000001E-9</td></tr>\n",
       "<tr><td>1</td><td>2022-11-02 15:38:12</td><td>2022-11-02 17:03:29</td><td>3.0</td><td>3.328626689185743</td><td>1.0</td><td>0</td><td>132</td><td>25</td><td>1</td><td>84.5</td><td>1.25</td><td>0.5</td><td>3.1179499067207184</td><td>0.0</td><td>0.3</td><td>108.15</td><td>0.0</td><td>1.25</td><td>4.457636454795698</td><td>15</td><td>4</td><td>17</td><td>4</td><td>1</td><td>403.5</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>4.127134385206382</td><td>9.999999950000001E-9</td></tr>\n",
       "<tr><td>1</td><td>2022-11-02 15:59:58</td><td>2022-11-02 16:12:16</td><td>1.0</td><td>0.6931471855599453</td><td>1.0</td><td>0</td><td>246</td><td>100</td><td>1</td><td>8.5</td><td>2.5</td><td>0.5</td><td>1.2089603488220497</td><td>0.0</td><td>0.3</td><td>14.15</td><td>2.5</td><td>0.0</td><td>2.5877640359795877</td><td>15</td><td>4</td><td>16</td><td>4</td><td>1</td><td>15.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>4.127134385206382</td><td>9.999999950000001E-9</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------+--------------------+---------------------+---------------+------------------+----------+------------------+------------+------------+------------+-----------+-----+-------+--------------------+------------+---------------------+------------+--------------------+-----------+------------------+-----------+----------------+------------+-----------------+---------------------+-------------------------+---------------+---------------+-----------------+------------------+----------------------------+-----------------------------+-----------------+------------+-----------------+--------------------+\n",
       "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|     trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|          tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|trip_duration_mins|pickup_hour|pickup_dayofweek|dropoff_hour|dropoff_dayofweek|days_since_2022_11_01|distance_time_interaction|is_airport_trip|is_tourist_trip|pickup_at_airport|dropoff_at_airport|pickup_at_tourist_attraction|dropoff_at_tourist_attraction|is_holiday_season|is_event_day|         avg_temp|       precipitation|\n",
       "+--------+--------------------+---------------------+---------------+------------------+----------+------------------+------------+------------+------------+-----------+-----+-------+--------------------+------------+---------------------+------------+--------------------+-----------+------------------+-----------+----------------+------------+-----------------+---------------------+-------------------------+---------------+---------------+-----------------+------------------+----------------------------+-----------------------------+-----------------+------------+-----------------+--------------------+\n",
       "|       1| 2022-11-01 15:09:19|  2022-11-01 15:14:39|            1.0|0.2623642721597987|       1.0|                 0|         161|         230|           1|        5.0|  2.5|    0.5|9.999999950000001E-9|         0.0|                  0.3|         8.3|                 2.5|        0.0|1.8458266920772781|         15|               3|          15|                3|                    0|                      4.5|              0|              1|                0|                 0|                           1|                            1|                0|           0|4.182050142793878| 0.19062036787311248|\n",
       "|       1| 2022-11-01 16:55:56|  2022-11-01 17:06:46|            2.0|0.7419373494912821|       1.0|                 0|         162|         100|           1|        8.0|  3.5|    0.5|  1.2383742339418191|         0.0|                  0.3|       14.75|                 2.5|        0.0|2.4709204086583307|         16|               3|          17|                3|                    0|                     17.6|              0|              1|                0|                 0|                           1|                            0|                0|           0|4.182050142793878| 0.19062036787311248|\n",
       "|       1| 2022-11-01 18:03:50|  2022-11-01 18:35:54|            1.0|1.5260563056689624|       1.0|                 0|         263|          48|           1|       20.0|  3.5|    0.5|   1.163150812930681|         0.0|                  0.3|        26.5|                 2.5|        0.0|3.4985257259251368|         18|               3|          18|                3|                    0|                     64.8|              0|              0|                0|                 0|                           0|                            0|                0|           0|4.182050142793878| 0.19062036787311248|\n",
       "|       1| 2022-11-01 18:14:45|  2022-11-01 18:34:13|            2.0| 0.993251776713987|       1.0|                 0|         237|         230|           1|       13.0|  3.5|    0.5|0.009950340754158134|         0.0|                  0.3|       17.31|                 2.5|        0.0|3.0187975469735866|         18|               3|          18|                3|                    0|       30.599999999999998|              0|              1|                0|                 0|                           0|                            1|                0|           0|4.182050142793878| 0.19062036787311248|\n",
       "|       1| 2022-11-01 18:38:02|  2022-11-01 19:00:26|            1.0|1.3083328223528814|       1.0|                 0|         234|          50|           2|       15.5|  3.5|    0.5|9.999999950000001E-9|         0.0|                  0.3|        19.8|                 2.5|        0.0|3.1527360227910064|         18|               3|          19|                3|                    0|                     48.6|              0|              0|                0|                 0|                           0|                            0|                0|           0|4.182050142793878| 0.19062036787311248|\n",
       "|       1| 2022-11-01 19:28:00|  2022-11-01 19:44:28|            1.0| 0.993251776713987|       1.0|                 0|         170|          68|           1|       11.5|  3.5|    0.5|   1.599387578600801|         0.0|                  0.3|       19.75|                 2.5|        0.0| 2.860294303231406|         19|               3|          19|                3|                    0|                     32.3|              0|              0|                0|                 0|                           0|                            0|                0|           0|4.182050142793878| 0.19062036787311248|\n",
       "|       1| 2022-11-01 23:41:47|  2022-11-01 23:53:26|            1.0| 1.098612292001443|       1.0|                 0|         152|          74|           1|       10.0|  0.5|    0.5|   1.335001069363919|         0.0|                  0.3|        14.1|                 0.0|        0.0|2.5376572159640434|         23|               3|          23|                3|                    0|                     46.0|              0|              0|                0|                 0|                           0|                            0|                0|           0|4.182050142793878| 0.19062036787311248|\n",
       "|       1| 2022-11-02 00:22:14|  2022-11-02 00:37:21|            1.0| 1.131402114716907|       1.0|                 0|         230|         158|           2|        9.0|  3.0|    0.5|9.999999950000001E-9|         0.0|                  0.3|        12.8|                 2.5|        0.0|2.7798539338516695|          0|               4|           0|                4|                    1|                      0.0|              0|              1|                0|                 0|                           1|                            0|                0|           0|4.127134385206382|9.999999950000001E-9|\n",
       "|       1| 2022-11-02 07:08:00|  2022-11-02 07:29:05|            1.0|1.3083328223528814|       1.0|                 0|         107|         140|           1|       14.5|  2.5|    0.5|  0.6931471855599453|         0.0|                  0.3|        18.8|                 2.5|        0.0| 3.094823176651052|          7|               4|           7|                4|                    1|       18.900000000000002|              0|              0|                0|                 0|                           0|                            0|                0|           0|4.127134385206382|9.999999950000001E-9|\n",
       "|       1| 2022-11-02 08:13:38|  2022-11-02 08:41:56|            1.0|1.7578579192765116|       1.0|                 0|         163|         261|           1|       20.0|  2.5|    0.5|  1.3862943636198906|         0.0|                  0.3|        26.3|                 2.5|        0.0|3.3775875163643185|          8|               4|           8|                4|                    1|                     38.4|              0|              1|                0|                 0|                           1|                            0|                0|           0|4.127134385206382|9.999999950000001E-9|\n",
       "|       1| 2022-11-02 08:38:53|  2022-11-02 08:57:15|            1.0|0.7419373494912821|       1.0|                 0|         229|         161|           1|       11.5|  2.5|    0.5|9.999999950000001E-9|         0.0|                  0.3|        14.8|                 2.5|        0.0| 2.963553375706107|          8|               4|           8|                4|                    1|                      8.8|              0|              1|                0|                 0|                           0|                            1|                0|           0|4.127134385206382|9.999999950000001E-9|\n",
       "|       1| 2022-11-02 08:57:20|  2022-11-02 09:15:21|            1.0| 1.335001069363919|       1.0|                 0|         238|          48|           1|       13.5|  2.5|    0.5|  1.4701758473994433|         0.0|                  0.3|       20.15|                 2.5|        0.0|2.9453157881658294|          8|               4|           9|                4|                    1|                     22.4|              0|              0|                0|                 0|                           0|                            0|                0|           0|4.127134385206382|9.999999950000001E-9|\n",
       "|       1| 2022-11-02 09:04:41|  2022-11-02 09:16:56|            1.0|0.7419373494912821|       1.0|                 0|         234|         230|           1|        8.5|  2.5|    0.5|  1.2119409769513032|         0.0|                  0.3|       14.16|                 2.5|        0.0|2.5839975531869483|          9|               4|           9|                4|                    1|                      9.9|              0|              1|                0|                 0|                           0|                            1|                0|           0|4.127134385206382|9.999999950000001E-9|\n",
       "|       1| 2022-11-02 09:45:19|  2022-11-02 10:01:59|            1.0| 1.163150812930681|       1.0|                 0|         239|         237|           1|       12.5|  2.5|    0.5|  1.4231083366522455|         0.0|                  0.3|       18.95|                 2.5|        0.0|  2.87167962545005|          9|               4|          10|                4|                    1|                     19.8|              0|              0|                0|                 0|                           0|                            0|                0|           0|4.127134385206382|9.999999950000001E-9|\n",
       "|       1| 2022-11-02 11:07:32|  2022-11-02 11:31:55|            1.0| 1.098612292001443|       1.0|                 0|         140|         143|           1|       15.5|  2.5|    0.5|  0.6931471855599453|         0.0|                  0.3|        19.8|                 2.5|        0.0|3.2340927910670203|         11|               4|          11|                4|                    1|                     22.0|              0|              0|                0|                 0|                           0|                            0|                0|           0|4.127134385206382|9.999999950000001E-9|\n",
       "|       1| 2022-11-02 12:19:31|  2022-11-02 12:32:12|            2.0|1.0296194207525868|       1.0|                 0|         140|         229|           3|       10.0|  2.5|    0.5|9.999999950000001E-9|         0.0|                  0.3|        13.3|                 2.5|        0.0|2.6161785479611437|         12|               4|          12|                4|                    1|                     21.6|              0|              0|                0|                 0|                           0|                            0|                0|           0|4.127134385206382|9.999999950000001E-9|\n",
       "|       1| 2022-11-02 14:27:19|  2022-11-02 14:51:20|            2.0|1.2527629713525108|       1.0|                 0|          43|         164|           1|       16.0|  2.5|    0.5|0.029558811950282222|         0.0|                  0.3|       19.33|                 2.5|        0.0| 3.219542269811095|         14|               4|          14|                4|                    1|                     35.0|              0|              1|                0|                 0|                           1|                            1|                0|           0|4.127134385206382|9.999999950000001E-9|\n",
       "|       1| 2022-11-02 15:31:47|  2022-11-02 15:56:04|            1.0|1.3083328223528814|       1.0|                 0|         100|         140|           1|       16.0|  2.5|    0.5|   1.098612292001443|         0.0|                  0.3|        21.3|                 2.5|        0.0|3.2301454175219493|         15|               4|          15|                4|                    1|                     40.5|              0|              0|                0|                 0|                           0|                            0|                0|           0|4.127134385206382|9.999999950000001E-9|\n",
       "|       1| 2022-11-02 15:38:12|  2022-11-02 17:03:29|            3.0| 3.328626689185743|       1.0|                 0|         132|          25|           1|       84.5| 1.25|    0.5|  3.1179499067207184|         0.0|                  0.3|      108.15|                 0.0|       1.25| 4.457636454795698|         15|               4|          17|                4|                    1|                    403.5|              1|              0|                1|                 0|                           0|                            0|                0|           0|4.127134385206382|9.999999950000001E-9|\n",
       "|       1| 2022-11-02 15:59:58|  2022-11-02 16:12:16|            1.0|0.6931471855599453|       1.0|                 0|         246|         100|           1|        8.5|  2.5|    0.5|  1.2089603488220497|         0.0|                  0.3|       14.15|                 2.5|        0.0|2.5877640359795877|         15|               4|          16|                4|                    1|                     15.0|              0|              0|                0|                 0|                           0|                            0|                0|           0|4.127134385206382|9.999999950000001E-9|\n",
       "+--------+--------------------+---------------------+---------------+------------------+----------+------------------+------------+------------+------------+-----------+-----+-------+--------------------+------------+---------------------+------------+--------------------+-----------+------------------+-----------+----------------+------------+-----------------+---------------------+-------------------------+---------------+---------------+-----------------+------------------+----------------------------+-----------------------------+-----------------+------------+-----------------+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Evaluators\n",
    "\n",
    "We set up evaluators to assess the performance of our models using metrics such as RMSE, MAE, and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluators for RMSE, MAE, and R-squared\n",
    "rmse_evaluator = RegressionEvaluator(labelCol='fare_amount', predictionCol='prediction', metricName='rmse')\n",
    "mae_evaluator = RegressionEvaluator(labelCol='fare_amount', predictionCol='prediction', metricName='mae')\n",
    "r2_evaluator = RegressionEvaluator(labelCol='fare_amount', predictionCol='prediction', metricName='r2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: 4393759 rows, 36 columns.\n"
     ]
    }
   ],
   "source": [
    "shape(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'tpep_pickup_datetime',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'RatecodeID',\n",
       " 'store_and_fwd_flag',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'congestion_surcharge',\n",
       " 'airport_fee',\n",
       " 'trip_duration_mins',\n",
       " 'pickup_hour',\n",
       " 'pickup_dayofweek',\n",
       " 'dropoff_hour',\n",
       " 'dropoff_dayofweek',\n",
       " 'days_since_2022_11_01',\n",
       " 'distance_time_interaction',\n",
       " 'is_airport_trip',\n",
       " 'is_tourist_trip',\n",
       " 'pickup_at_airport',\n",
       " 'dropoff_at_airport',\n",
       " 'pickup_at_tourist_attraction',\n",
       " 'dropoff_at_tourist_attraction',\n",
       " 'is_holiday_season',\n",
       " 'is_event_day',\n",
       " 'avg_temp',\n",
       " 'precipitation']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection for Linear Model\n",
    "\n",
    "In the linear model, the following columns were selected for use:\n",
    "\n",
    "- **VendorID**\n",
    "- **trip_distance**\n",
    "- **RatecodeID**\n",
    "- **PULocationID**\n",
    "- **DOLocationID**\n",
    "- **mta_tax**\n",
    "- **tolls_amount**\n",
    "- **improvement_surcharge**\n",
    "- **congestion_surcharge**\n",
    "- **airport_fee**\n",
    "- **pickup_hour**\n",
    "- **pickup_dayofweek**\n",
    "- **days_since_2022_11_01**\n",
    "- **dropoff_at_airport**\n",
    "- **pickup_at_tourist_attraction**\n",
    "- **dropoff_at_tourist_attraction**\n",
    "- **is_holiday_season**\n",
    "- **is_event_day**\n",
    "- **avg_temp**\n",
    "\n",
    "1. **Avoidance of Multicollinearity**:\n",
    "   - **Multicollinearity** occurs when two or more features are highly correlated, which can distort the results of a linear model by making it difficult to isolate the effect of individual predictors.\n",
    "   - For example, `fare_amount`, `trip_duration_mins`, and `trip_distance` are strongly correlated with each other. Including all of them could lead to unreliable coefficients and inflated standard errors.\n",
    "   - To address this, highly correlated variables were excluded, providing more granular insights without introducing multicollinearity.\n",
    "\n",
    "2. **Retaining Key Predictors**:\n",
    "   - Features like `trip_distance`, `pickup_hour`, and `RatecodeID` were retained as they are critical in predicting the target variable (such as fare or trip duration) and are not strongly correlated with each other.\n",
    "   - Variables such as `pickup_at_tourist_attraction` and `dropoff_at_airport` were included to capture location-specific effects, especially relevant for tourists, without overlapping with other variables.\n",
    "\n",
    "3. **Inclusion of Temporal and Contextual Variables**:\n",
    "   - **Temporal variables** like `pickup_hour`, `pickup_dayofweek`, and `days_since_2022_11_01` are included to account for time-based variations in the data.\n",
    "   - **Contextual variables** such as `is_holiday_season`, `is_event_day`, and `avg_temp` help capture broader influences on taxi demand and fare variability.\n",
    "\n",
    "=> By selecting these features, the model is designed to be more interpretable and reliable, minimizing issues related to multicollinearity while capturing the essential factors that influence the target variable. This approach ensures that the model coefficients are meaningful and that the model performs better in predicting outcomes based on the selected variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the relevant columns to include in the model\n",
    "relevant_columns = ['VendorID',\n",
    "                    'trip_distance',\n",
    "                    'RatecodeID',\n",
    "                    'PULocationID',\n",
    "                    'DOLocationID',\n",
    "                    'mta_tax',\n",
    "                    'tolls_amount',\n",
    "                    'improvement_surcharge',\n",
    "                    'congestion_surcharge',\n",
    "                    'airport_fee',\n",
    "                    'pickup_hour',\n",
    "                    'pickup_dayofweek',\n",
    "                    'days_since_2022_11_01',\n",
    "                    'dropoff_at_airport',\n",
    "                    'pickup_at_tourist_attraction',\n",
    "                    'dropoff_at_tourist_attraction',\n",
    "                    'is_holiday_season',\n",
    "                    'is_event_day',\n",
    "                    'avg_temp',\n",
    "                    ]\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_end_date = '2023-03-31'\n",
    "test_start_date = '2023-04-01'\n",
    "\n",
    "train_data = df_train.filter(\n",
    "    (to_date(col('tpep_pickup_datetime')) >= '2022-11-01') & \n",
    "    (to_date(col('tpep_pickup_datetime')) <= train_end_date)\n",
    ")\n",
    "\n",
    "test_data = df_train.filter(\n",
    "    (to_date(col('tpep_pickup_datetime')) >= test_start_date) & \n",
    "    (to_date(col('tpep_pickup_datetime')) <= '2023-04-30')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop('tpep_pickup_datetime', 'tpep_dropoff_datetime')\n",
    "test_data = test_data.drop('tpep_pickup_datetime', 'tpep_dropoff_datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Assembly and Scaling\n",
    "\n",
    "We use `VectorAssembler` to combine the relevant columns into a single feature vector and then apply `StandardScaler` to standardize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the relevant features into a single feature vector\n",
    "fare_assembler = VectorAssembler(inputCols=relevant_columns, outputCol='unscaled_features')\n",
    "fare_train_assembled = fare_assembler.transform(train_data)\n",
    "fare_test_assembled = fare_assembler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 20:23:31 ERROR Executor: Exception in task 0.0 in stage 13.0 (TID 30)1]\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.copy(UnsafeRow.java:469)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.copy(UnsafeRow.java:57)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$doExecute$13(limit.scala:338)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$Lambda$3699/63088991.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:33)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:665)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:42)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$doExecute$12(limit.scala:338)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$Lambda$3689/1972100888.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2742/1829898525.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "24/08/25 20:23:31 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 13.0 (TID 30),5,main]\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.copy(UnsafeRow.java:469)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.copy(UnsafeRow.java:57)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$doExecute$13(limit.scala:338)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$Lambda$3699/63088991.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:33)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:665)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:42)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$doExecute$12(limit.scala:338)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$Lambda$3689/1972100888.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2742/1829898525.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "24/08/25 20:23:31 WARN TaskSetManager: Lost task 0.0 in stage 13.0 (TID 30) (10.13.11.182 executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.copy(UnsafeRow.java:469)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.copy(UnsafeRow.java:57)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$doExecute$13(limit.scala:338)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$Lambda$3699/63088991.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:33)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:665)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:42)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$doExecute$12(limit.scala:338)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$Lambda$3689/1972100888.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2742/1829898525.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\n",
      "24/08/25 20:23:31 ERROR TaskSetManager: Task 0 in stage 13.0 failed 1 times; aborting job\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o94.fit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Apply standard scaling to the feature vector\u001b[39;00m\n\u001b[1;32m      2\u001b[0m fare_scaler \u001b[38;5;241m=\u001b[39m StandardScaler(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munscaled_features\u001b[39m\u001b[38;5;124m'\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaled_features\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m fare_scaler_model \u001b[38;5;241m=\u001b[39m \u001b[43mfare_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfare_train_assembled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m fare_train_scaled \u001b[38;5;241m=\u001b[39m fare_scaler_model\u001b[38;5;241m.\u001b[39mtransform(fare_train_assembled)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaled_features\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfare_amount\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m fare_test_scaled \u001b[38;5;241m=\u001b[39m fare_scaler_model\u001b[38;5;241m.\u001b[39mtransform(fare_test_assembled)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaled_features\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfare_amount\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o94.fit"
     ]
    }
   ],
   "source": [
    "# Apply standard scaling to the feature vector\n",
    "fare_scaler = StandardScaler(inputCol='unscaled_features', outputCol='scaled_features')\n",
    "fare_scaler_model = fare_scaler.fit(fare_train_assembled)\n",
    "fare_train_scaled = fare_scaler_model.transform(fare_train_assembled).select('scaled_features', 'fare_amount')\n",
    "fare_test_scaled = fare_scaler_model.transform(fare_test_assembled).select('scaled_features', 'fare_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_train_scaled.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model\n",
    "\n",
    "We build a standard linear regression model using the scaled features, train the model on the training set, and evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear regression model on the scaled features\n",
    "lr = LinearRegression(featuresCol='scaled_features', labelCol='fare_amount')\n",
    "lr_model = lr.fit(fare_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained modelfor quick retrieval\n",
    "lr_model.save(\"/Users/jennymai/Desktop/data_sci/mast_project1/models/linear_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set and print the performance metrics\n",
    "lr_predictions = lr_model.transform(fare_test_scaled)\n",
    "\n",
    "lr_rmse = rmse_evaluator.evaluate(lr_predictions)\n",
    "lr_mae = mae_evaluator.evaluate(lr_predictions)\n",
    "lr_r2 = r2_evaluator.evaluate(lr_predictions)\n",
    "print(f\"Linear Regression on test_data - RMSE: {lr_rmse}, MAE: {lr_mae}, R2: {lr_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals Analysis for Linear Regression\n",
    "\n",
    "We analyze the residuals of the linear regression model by plotting the residuals against the fitted values, plotting the histogram of residuals, and creating a Q-Q plot to check for normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals and convert to Pandas DataFrame for analysis\n",
    "predictions_and_residuals = lr_predictions.select(\"prediction\", \"fare_amount\").withColumn(\"residuals\", lr_predictions[\"fare_amount\"] - lr_predictions[\"prediction\"])\n",
    "predictions_and_residuals_pd = predictions_and_residuals.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals vs fitted values\n",
    "plt.scatter(predictions_and_residuals_pd['prediction'], predictions_and_residuals_pd['residuals'], alpha=0.2)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.xlabel('Fitted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Linear Model: Residuals vs Fitted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(predictions_and_residuals_pd['prediction'], predictions_and_residuals_pd['residuals'], alpha=0.2)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.xlabel('Fitted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Linear Model: Residuals vs Fitted Values (Range 0-25)')\n",
    "plt.xlim(0, 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Overall Residuals Pattern**:\n",
    "   - The first plot shows residuals across the full range of fitted values. There is a visible funnel shape, where residuals start small and become more dispersed as the fitted values increase. This pattern indicates heteroscedasticity, where the variance of the residuals increases with the fitted values, which can violate the assumptions of linear regression.\n",
    "\n",
    "2. **Focused Range (0-25)**:\n",
    "   - The second plot focuses on the range of fitted values between 0 and 25. Here, the residuals are more tightly clustered around the zero line, indicating that the model fits reasonably well in this lower range.\n",
    "   - However, there are still some points with relatively large residuals, suggesting potential outliers or non-linear relationships that the model might not be capturing effectively.\n",
    "\n",
    "3. **Implications**:\n",
    "   - **Heteroscedasticity**: The increasing spread of residuals at higher fitted values suggests that the model's accuracy decreases as the predicted values increase. This could indicate that the model is not well-suited for predicting higher fare amounts and that a different model or transformation might be needed to better handle this range.\n",
    "   - **Potential Non-Linearity**: The presence of non-zero residuals across all ranges suggests that the relationship between the predictors and the target variable might not be perfectly linear. This could mean that the model might benefit from including interaction terms, polynomial features, or even switching to a non-linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of residuals\n",
    "sns.histplot(predictions_and_residuals_pd['residuals'], kde=True)\n",
    "plt.xlabel('Residuals')\n",
    "plt.title('Linear Model: Histogram of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(predictions_and_residuals_pd['residuals'], kde=True)\n",
    "plt.xlabel('Residuals')\n",
    "plt.title('Linear Model: Histogram of Residuals (Range -10-20)')\n",
    "plt.xlim(-10,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Overall Distribution**:\n",
    "   - The first histogram shows the full range of residuals, with most values clustering near zero but with a noticeable right-skew. This skew indicates that the model tends to underpredict in some cases, resulting in positive residuals.\n",
    "\n",
    "2. **Focused Range (-10 to 20)**:\n",
    "   - The second histogram zooms in on a more focused range (-10 to 20), where the bulk of residuals lie. The distribution here appears more normal, but the right skew is still evident.\n",
    "   - The presence of a peak around zero is expected and desirable, indicating that the model performs well for a large portion of the data. However, the right tail suggests that there are instances where the model significantly underestimates the actual values.\n",
    "\n",
    "3. **Non-Normality of Residuals**: The right skew in the residuals suggests that the assumptions of normality required for a linear regression model might not be fully met. This could impact the accuracy of confidence intervals and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Q plot of residuals to check normality\n",
    "probplot(predictions_and_residuals_pd['residuals'], dist=\"norm\", plot=plt)\n",
    "plt.title('Linear Model: Q-Q Plot of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Deviations from Normality**:\n",
    "   - The Q-Q plot shows that the residuals significantly deviate from the straight line, particularly in the tails. This indicates that the residuals are not normally distributed, with pronounced skewness.\n",
    "   - The right tail of the plot (positive residuals) deviates upward, showing that the model underpredicts for some higher values, resulting in large positive residuals.\n",
    "\n",
    "2. **Implications for Model Assumptions**:\n",
    "   - **Non-Normal Residuals**: The pronounced departure from the normality line suggests that the assumption of normally distributed residuals, a key assumption in linear regression, is violated.\n",
    "   - **Potential Impact on Inference**: Because the residuals are not normally distributed, this can affect the reliability of confidence intervals and hypothesis tests, potentially leading to inaccurate predictions or conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Inflation Factor (VIF)\n",
    "\n",
    "We calculate the Variance Inflation Factor (VIF) to check for multicollinearity among the predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample of the training data into a Pandas DataFrame for VIF calculation\n",
    "pd_df_train = pd.read_parquet('/Users/jennymai/Desktop/data_sci/mast_project1/data/development').sample(frac=0.1, random_state=1003)\n",
    "X = pd_df_train[relevant_columns]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Durbin-Watson Statistic\n",
    "\n",
    "We calculate the Durbin-Watson statistic to test for autocorrelation in the residuals of the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Durbin-Watson statistic for the residuals\n",
    "dw_statistic = durbin_watson(predictions_and_residuals_pd['residuals'])\n",
    "print(f'Durbin-Watson statistic: {dw_statistic:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Variance Inflation Factor (VIF) Analysis**:\n",
    "   - **VIF**: VIF quantifies the severity of multicollinearity in a regression analysis. A VIF value above 5 indicates a high level of multicollinearity, while values below 5 are generally acceptable.\n",
    "   - **Key Observations**:\n",
    "     - All features have VIF values well below the critical threshold of 5, with most values close to 1. This suggests that multicollinearity is not a significant issue in the selected model features.\n",
    "     - The highest VIF values are seen in `RatecodeID` (2.278) and `airport_fee` (2.309), but these values are still within acceptable limits, indicating that these features do not introduce problematic multicollinearity.\n",
    "\n",
    "2. **Durbin-Watson Statistic**:\n",
    "   - **Durbin-Watson**: The Durbin-Watson statistic tests for autocorrelation in the residuals from a statistical regression analysis. Values close to 2 indicate no autocorrelation, while values closer to 0 suggest positive autocorrelation.\n",
    "   - **Key Observation**:\n",
    "     - The Durbin-Watson statistic for this model is 1.645, which is close to 2, indicating that there is no significant autocorrelation in the residuals. This suggests that the residuals are independent, a favorable outcome for the linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficients of Linear Regression\n",
    "\n",
    "We print the coefficients of the trained linear regression model to understand the impact of each feature on the predicted fare amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coefficients of the linear regression model\n",
    "coefficients = lr_model.coefficients\n",
    "\n",
    "for feature, coeff in zip(relevant_columns, coefficients):\n",
    "    print(f\"Feature: {feature}, Coefficient: {coeff:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dominant Factors**: The most significant positive contributors to fare amount are trip distance, RatecodeID, and tolls, reflecting expected fare components.\n",
    "- **Negative Effects**: Certain features, like VendorID and mta_tax, show small negative coefficients, which might indicate adjustments or discounts in specific cases.\n",
    "- **Complex Interactions**: Some coefficients, like the congestion surcharge, suggest complex interactions that might require further exploration to fully understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression Model\n",
    "\n",
    "We train a Lasso regression model, which includes L1 regularization to penalize the magnitude of the coefficients. This model is trained on the scaled features and evaluated on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Lasso regression model with L1 regularization\n",
    "lasso = LinearRegression(featuresCol='scaled_features', labelCol='fare_amount', elasticNetParam=0.5, regParam=0.1)\n",
    "lasso_model = lasso.fit(fare_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Lasso model\n",
    "lasso_model.save(\"/Users/jennymai/Desktop/data_sci/mast_project1/models/lasso_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Lasso model on the test set and print the performance metrics\n",
    "lasso_test_predictions = lasso_model.transform(fare_test_scaled)\n",
    "\n",
    "lasso_test_rmse = rmse_evaluator.evaluate(lasso_test_predictions)\n",
    "lasso_test_mae = mae_evaluator.evaluate(lasso_test_predictions)\n",
    "lasso_test_r2 = r2_evaluator.evaluate(lasso_test_predictions)\n",
    "print(f\"Lasso Regression on test_data - RMSE: {lasso_test_rmse}, MAE: {lasso_test_mae}, R2: {lasso_test_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals Analysis for Lasso Regression\n",
    "\n",
    "Similar to the linear regression model, we analyze the residuals of the Lasso regression model by plotting the residuals against the fitted values, plotting the histogram of residuals, and creating a Q-Q plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals for the Lasso model and convert to Pandas DataFrame for analysis\n",
    "predictions_and_residuals_lasso = lasso_test_predictions.select(\"prediction\", \"fare_amount\").withColumn(\"residuals\", lasso_test_predictions[\"fare_amount\"] - lasso_test_predictions[\"prediction\"])\n",
    "predictions_and_residuals_lasso_pd = predictions_and_residuals.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals vs fitted values for the Lasso model\n",
    "plt.scatter(predictions_and_residuals_lasso_pd['prediction'], predictions_and_residuals_lasso_pd['residuals'], alpha=0.2)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.xlabel('Fitted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Lasso Model: Residuals vs Fitted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of residuals for the Lasso model\n",
    "sns.histplot(predictions_and_residuals_lasso_pd['residuals'], kde=True)\n",
    "plt.xlabel('Residuals')\n",
    "plt.title('Lasso Model: Histogram of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Q plot of residuals for the Lasso model to check normality\n",
    "probplot(predictions_and_residuals_lasso_pd['residuals'], dist=\"norm\", plot=plt)\n",
    "plt.title('Lasso Model: Q-Q Plot of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Residuals vs. Fitted Values**:\n",
    "   - The residuals vs. fitted values plot shows a similar pattern to the linear regression model, with residuals clustering around zero at lower fitted values and spreading out as the fitted values increase. This indicates **heteroscedasticity**, where residuals have increasing variance at higher predicted values. The Lasso model appears to struggle with predicting higher values accurately, leading to larger residuals.\n",
    "\n",
    "2. **Histogram of Residuals**:\n",
    "   - The histogram of residuals shows a right-skewed distribution, where most residuals are close to zero, but a significant number are positive, indicating underprediction. The distribution is not perfectly normal, with a notable skew to the right, similar to what was observed in the linear regression model.\n",
    "\n",
    "3. **Q-Q Plot of Residuals**:\n",
    "   - The Q-Q plot reveals that the residuals deviate from the normal distribution, particularly in the tails. The right tail, representing large positive residuals, deviates upward, showing that the model underpredicts in certain cases, leading to residuals that are much larger than would be expected under a normal distribution.\n",
    "\n",
    "- **Heteroscedasticity**: The increasing spread of residuals with larger fitted values suggests that the Lasso model, like the linear model, may not adequately capture relationships at higher values.\n",
    "- **Non-Normal Residuals**: The right-skew in both the histogram and Q-Q plot indicates that the residuals are not normally distributed, which could affect the reliability of inferences made from the model.\n",
    "- **Lasso Regularization Impact**: Despite the regularization introduced by the Lasso model, which penalizes large coefficients and can reduce overfitting, the model still exhibits significant issues with heteroscedasticity and non-normal residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficients of Lasso Regression\n",
    "\n",
    "We print the coefficients of the trained Lasso regression model to understand the impact of each feature on the predicted fare amount, especially focusing on which features are penalized (coefficients closer to zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coefficients of the Lasso regression model\n",
    "coefficients = lasso_model.coefficients\n",
    "\n",
    "for feature, coeff in zip(relevant_columns, coefficients):\n",
    "    print(f\"Feature: {feature}, Coefficient: {coeff:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Using Backward Elimination\n",
    "\n",
    "We implement a backward elimination procedure to identify the most significant features for predicting fare amounts by iteratively removing the least significant feature based on the magnitude of its coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_elimination(data: DataFrame, features: list, label: str, stop_threshold=0.1):\n",
    "    features_to_keep = features.copy()\n",
    "    \n",
    "    while len(features_to_keep) > 0:\n",
    "        print(f\"Training model with {len(features_to_keep)} features.\")\n",
    "        \n",
    "        # Assemble the feature vector for current set of features\n",
    "        assembler = VectorAssembler(inputCols=features_to_keep, outputCol=\"features\")\n",
    "        data_assembled = assembler.transform(data).select(\"features\", label)\n",
    "        \n",
    "        # Train the model\n",
    "        lr = LinearRegression(featuresCol=\"features\", labelCol=label)\n",
    "        lr_model = lr.fit(data_assembled)\n",
    "        \n",
    "        # Get the coefficients and associated features\n",
    "        coefficients = lr_model.coefficients\n",
    "        coef_feature_pairs = list(zip(coefficients, features_to_keep))\n",
    "        \n",
    "        # Find the least significant feature (smallest coefficient magnitude)\n",
    "        least_significant_feature = min(coef_feature_pairs, key=lambda x: abs(x[0]))[1]\n",
    "        \n",
    "        # Check the magnitude of the smallest coefficient\n",
    "        if abs(min(coef_feature_pairs, key=lambda x: abs(x[0]))[0]) < stop_threshold:\n",
    "            print(f\"Removing least significant feature: {least_significant_feature}\")\n",
    "            features_to_keep.remove(least_significant_feature)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    print(f\"Final set of features: {features_to_keep}\")\n",
    "    return features_to_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected Features After Backward Elimination\n",
    "\n",
    "We apply backward elimination on the training data to determine the final set of features and then re-train and evaluate a linear regression model using these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply backward elimination to select the most significant features\n",
    "selected_features = backward_elimination(data=train_data, features=relevant_columns, label='fare_amount', stop_threshold=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the final set of selected features for training and test sets\n",
    "final_assembler = VectorAssembler(inputCols=selected_features, outputCol=\"final_features\")\n",
    "train_final = final_assembler.transform(train_data)\n",
    "test_final = final_assembler.transform(test_data)\n",
    "\n",
    "# Scale the selected features\n",
    "are_scaler = StandardScaler(inputCol='final_features', outputCol='scaled_features')\n",
    "fare_scaler_model = fare_scaler.fit(train_final)\n",
    "fare_train_scaled = fare_scaler_model.transform(train_final).select('scaled_features', 'fare_amount')\n",
    "fare_test_scaled = fare_scaler_model.transform(test_final).select('scaled_features', 'fare_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a final linear regression model using the selected features\n",
    "final_lr = LinearRegression(featuresCol='scaled_features', labelCol='fare_amount')\n",
    "final_model = final_lr.fit(fare_train_scaled)\n",
    "\n",
    "# Evaluate the final model on the test set and print the performance metrics\n",
    "test_predictions = final_model.transform(fare_test_scaled)\n",
    "test_rmse = rmse_evaluator.evaluate(test_predictions)\n",
    "test_mae = mae_evaluator.evaluate(test_predictions)\n",
    "test_r2 = r2_evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(f\"Final Model on Test Data -> RMSE: {test_rmse}, MAE: {test_mae}, R2: {test_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final linear regression model\n",
    "final_model.save(\"/Users/jennymai/Desktop/data_sci/mast_project1/models/be_linear_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session to release resources\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
