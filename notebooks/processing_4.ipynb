{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Data Integration and Duplicate Removal\n",
    "\n",
    "This notebook integrates NYC weather data with taxi trip data and removes duplicate records. Key steps include:\n",
    "\n",
    "1. **Initialization**: Setting up a Spark session and loading required datasets.\n",
    "2. **Weather Data Cleaning**: Handling trace values, removing non-numeric characters, and ensuring consistent data types.\n",
    "3. **Data Integration**: Merging cleaned weather data with taxi trip data to incorporate average temperature and precipitation into the dataset.\n",
    "4. **Duplicate Identification**: Identifying and filtering out frequent duplicates based on key columns like fare amount, location IDs, and trip distance.\n",
    "5. **Final Data Cleaning**: Removing duplicates to produce a cleaned and unique dataset.\n",
    "6. **Saving Outputs**: Saving the final cleaned and integrated DataFrame to a new Parquet file for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Initial Setup\n",
    "\n",
    "We begin by importing the necessary libraries, initializing a Spark session, and loading the required datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, when, month, dayofmonth, weekofyear, date_format, lit, to_date, regexp_replace\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/25 12:36:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1\")  # Name the Spark application\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)  # Enable eager evaluation for interactive querying\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")  # Cache metadata for parquet files\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")  # Set the timezone to UTC\n",
    "    .getOrCreate()  # Create or retrieve the existing Spark session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape(sdf: DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Returns the shape of a Spark DataFrame as a tuple (number of rows, number of columns).\n",
    "\n",
    "    :param sdf: Spark DataFrame\n",
    "    :return: String stating the shape of sdf\n",
    "    \"\"\"\n",
    "    num_rows = sdf.count()\n",
    "    num_columns = len(sdf.columns)\n",
    "    print(f\"Shape of the DataFrame: {num_rows} rows, {num_columns} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Inspect Weather Data\n",
    "\n",
    "NYC Weather Data: https://www.weather.gov/wrh/Climate?wfo=okx\n",
    "\n",
    "The data is manually collected and converted into CSV\n",
    "\n",
    "Load the NYC weather data from a CSV file and inspect the first few rows and schema to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-------------------+---------+---+---+-------------+--------+----------+\n",
      "|      Date|Maximum Temperature|Minimum Temperature|Average Temperature|Departure|HDD|CDD|Precipitation|New Snow|Snow Depth|\n",
      "+----------+-------------------+-------------------+-------------------+---------+---+---+-------------+--------+----------+\n",
      "|2023-04-01|                 61|                 49|               55.0|      9.2| 10|  0|         0.22|     0.0|         0|\n",
      "|2023-04-02|                 52|                 37|               44.5|     -1.7| 20|  0|          0.0|     0.0|         0|\n",
      "|2023-04-03|                 53|                 33|               43.0|     -3.5| 22|  0|          0.0|     0.0|         0|\n",
      "|2023-04-04|                 64|                 43|               53.5|      6.7| 11|  0|          0.0|     0.0|         0|\n",
      "|2023-04-05|                 54|                 48|               51.0|      3.8| 14|  0|            T|     0.0|         0|\n",
      "|2023-04-06|                 74|                 48|               61.0|     13.5|  4|  0|         0.02|     0.0|         0|\n",
      "|2023-04-07|                 58|                 43|               50.5|      2.6| 14|  0|          0.0|     0.0|         0|\n",
      "|2023-04-08|                 51|                 38|               44.5|     -3.7| 20|  0|          0.0|     0.0|         0|\n",
      "|2023-04-09|                 51|                 38|               44.5|     -4.1| 20|  0|          0.0|     0.0|         0|\n",
      "|2023-04-10|                 58|                 38|               48.0|     -0.9| 17|  0|          0.0|     0.0|         0|\n",
      "+----------+-------------------+-------------------+-------------------+---------+---+---+-------------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load weather data from CSV file\n",
    "df_weather = spark.read.csv('/Users/jennymai/Desktop/data_sci/mast_project1/data/external_data/nyc_weather.csv', header=True, inferSchema=True)\n",
    "df_weather.limit(10).show()  # Display the first 10 rows of the weather dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Maximum Temperature: integer (nullable = true)\n",
      " |-- Minimum Temperature: integer (nullable = true)\n",
      " |-- Average Temperature: double (nullable = true)\n",
      " |-- Departure: double (nullable = true)\n",
      " |-- HDD: integer (nullable = true)\n",
      " |-- CDD: integer (nullable = true)\n",
      " |-- Precipitation: string (nullable = true)\n",
      " |-- New Snow: string (nullable = true)\n",
      " |-- Snow Depth: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of the weather DataFrame to understand its structure\n",
    "df_weather.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 12:37:01 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>summary</th><th>Maximum Temperature</th><th>Minimum Temperature</th><th>Average Temperature</th><th>Departure</th><th>HDD</th><th>CDD</th><th>Precipitation</th><th>New Snow</th><th>Snow Depth</th></tr>\n",
       "<tr><td>count</td><td>181</td><td>181</td><td>181</td><td>181</td><td>181</td><td>181</td><td>181</td><td>181</td><td>181</td></tr>\n",
       "<tr><td>mean</td><td>51.469613259668506</td><td>36.98342541436464</td><td>44.226519337016576</td><td>3.4718232044198905</td><td>20.640883977900554</td><td>0.11602209944751381</td><td>0.1260122699386503</td><td>0.009770114942528735</td><td>0.0111731843575419</td></tr>\n",
       "<tr><td>stddev</td><td>10.6016463273211</td><td>9.352643796228337</td><td>9.520799659220483</td><td>7.472150626880584</td><td>9.226669514258814</td><td>0.6349257870526734</td><td>0.2795378458383169</td><td>0.08579417514612549</td><td>0.14948701855038715</td></tr>\n",
       "<tr><td>min</td><td>16</td><td>4</td><td>11.5</td><td>-24.8</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td>0</td></tr>\n",
       "<tr><td>max</td><td>85</td><td>64</td><td>70.0</td><td>20.3</td><td>53</td><td>5</td><td>T</td><td>T</td><td>T</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+------------------+--------------------+-------------------+\n",
       "|summary|Maximum Temperature|Minimum Temperature|Average Temperature|         Departure|               HDD|                CDD|     Precipitation|            New Snow|         Snow Depth|\n",
       "+-------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+------------------+--------------------+-------------------+\n",
       "|  count|                181|                181|                181|               181|               181|                181|               181|                 181|                181|\n",
       "|   mean| 51.469613259668506|  36.98342541436464| 44.226519337016576|3.4718232044198905|20.640883977900554|0.11602209944751381|0.1260122699386503|0.009770114942528735| 0.0111731843575419|\n",
       "| stddev|   10.6016463273211|  9.352643796228337|  9.520799659220483| 7.472150626880584| 9.226669514258814| 0.6349257870526734|0.2795378458383169| 0.08579417514612549|0.14948701855038715|\n",
       "|    min|                 16|                  4|               11.5|             -24.8|                 0|                  0|               0.0|                 0.0|                  0|\n",
       "|    max|                 85|                 64|               70.0|              20.3|                53|                  5|                 T|                   T|                  T|\n",
       "+-------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+------------------+--------------------+-------------------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: 181 rows, 10 columns.\n"
     ]
    }
   ],
   "source": [
    "shape(df_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Weather Data for Anomalies\n",
    "\n",
    "Check for distinct values in the `Precipitation`, `New Snow`, and `Snow Depth` columns to identify any anomalies or unusual data entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|Precipitation|\n",
      "+-------------+\n",
      "|         0.32|\n",
      "|         0.11|\n",
      "|         0.03|\n",
      "|          0.3|\n",
      "|         0.31|\n",
      "|         0.14|\n",
      "|         0.25|\n",
      "|         0.36|\n",
      "|         0.68|\n",
      "|         0.06|\n",
      "|         1.02|\n",
      "|          0.2|\n",
      "|            T|\n",
      "|         0.21|\n",
      "|          0.0|\n",
      "|         0.96|\n",
      "|         0.08|\n",
      "|         0.51|\n",
      "|         0.02|\n",
      "|         0.22|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+\n",
      "|New Snow|\n",
      "+--------+\n",
      "|     1.0|\n",
      "|     0.2|\n",
      "|       T|\n",
      "|     0.0|\n",
      "|     0.5|\n",
      "+--------+\n",
      "\n",
      "+----------+\n",
      "|Snow Depth|\n",
      "+----------+\n",
      "|         0|\n",
      "|         T|\n",
      "|         2|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check distinct values in key columns to identify any anomalies or unusual entries\n",
    "df_weather.select(\"Precipitation\").distinct().show()\n",
    "df_weather.select(\"New Snow\").distinct().show()\n",
    "df_weather.select(\"Snow Depth\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice the special `T` value in the variables, which is defined as less than the smallest measurable amount.  That threshold is below for the different precipitation measurements:\n",
    "\n",
    "- Liquid precipitation (rain, showers) - Less than 0.005\"\n",
    "- Snowfall - Less than 0.05\"\n",
    "- Snow depth on the ground - Less than 0.5\"\n",
    "\n",
    "Source: https://www.weather.gov/climateservices/nowdatafaq "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "### Handle Trace Values in Weather Data\n",
    "\n",
    "Since T means a very small amount, we will replace 'T' (trace amounts) with 0 in the `Precipitation`, `New Snow`, and `Snow Depth` columns to simplify data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'T' (trace amounts) with 0 in key columns to clean the data\n",
    "df_weather = df_weather.withColumn(\"Precipitation\", when(col(\"Precipitation\") == 'T', 0).otherwise(col(\"Precipitation\"))) \\\n",
    "                       .withColumn(\"New Snow\", when(col(\"New Snow\") == 'T', 0).otherwise(col(\"New Snow\"))) \\\n",
    "                       .withColumn(\"Snow Depth\", when(col(\"Snow Depth\") == 'T', 0).otherwise(col(\"Snow Depth\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Non-Numeric Characters\n",
    "\n",
    "Further clean the weather data by removing any non-numeric characters from the `Precipitation`, `New Snow`, and `Snow Depth` columns and converting them to double data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Maximum Temperature: integer (nullable = true)\n",
      " |-- Minimum Temperature: integer (nullable = true)\n",
      " |-- Average Temperature: double (nullable = true)\n",
      " |-- Departure: double (nullable = true)\n",
      " |-- HDD: integer (nullable = true)\n",
      " |-- CDD: integer (nullable = true)\n",
      " |-- Precipitation: double (nullable = true)\n",
      " |-- New Snow: double (nullable = true)\n",
      " |-- Snow Depth: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove non-numeric characters and cast columns to double for consistent data types\n",
    "df_weather_cleaned = df_weather.withColumn(\"Precipitation\", regexp_replace(col(\"Precipitation\"), \"[^0-9.]\", \"\").cast(\"double\")) \\\n",
    "                                 .withColumn(\"New Snow\", regexp_replace(col(\"New Snow\"), \"[^0-9.]\", \"\").cast(\"double\")) \\\n",
    "                                 .withColumn(\"Snow Depth\", regexp_replace(col(\"Snow Depth\"), \"[^0-9.]\", \"\").cast(\"double\"))\n",
    "\n",
    "# Print the schema of the cleaned weather DataFrame\n",
    "df_weather_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: 181 rows, 10 columns.\n"
     ]
    }
   ],
   "source": [
    "shape(df_weather_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Curated Taxi Data\n",
    "\n",
    "Load the curated NYC taxi data from a Parquet file for further integration with the cleaned weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5016231"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the curated taxi data from a Parquet file\n",
    "sdf = spark.read.parquet('/Users/jennymai/Desktop/data_sci/mast_project1/data/curated2')\n",
    "sdf.count()  # Count the number of records in the DataFrame to understand its size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: 5016231 rows, 34 columns.\n"
     ]
    }
   ],
   "source": [
    "shape(sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate Weather Data with Taxi Data\n",
    "\n",
    "Add a date column to the taxi data and join it with the weather data to integrate daily weather conditions into the taxi dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+-----------+----------------+------------+-----------------+---------------------+-------------------------+---------------+---------------+-----------------+------------------+----------------------------+-----------------------------+-----------------+------------+--------+-------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|trip_duration_mins|pickup_hour|pickup_dayofweek|dropoff_hour|dropoff_dayofweek|days_since_2022_11_01|distance_time_interaction|is_airport_trip|is_tourist_trip|pickup_at_airport|dropoff_at_airport|pickup_at_tourist_attraction|dropoff_at_tourist_attraction|is_holiday_season|is_event_day|avg_temp|precipitation|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+-----------+----------------+------------+-----------------+---------------------+-------------------------+---------------+---------------+-----------------+------------------+----------------------------+-----------------------------+-----------------+------------+--------+-------------+\n",
      "|       1| 2022-11-01 00:51:22|  2022-11-01 00:56:24|            1.0|          0.6|       1.0|                 0|         151|         151|           2|        4.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         5.8|                 0.0|        0.0| 5.033333333333333|          0|               3|           0|                3|                    0|                      0.0|              0|              0|                0|                 0|                           0|                            0|                0|           0|    64.5|         0.21|\n",
      "|       1| 2022-11-01 00:24:49|  2022-11-01 00:31:04|            2.0|          1.0|       1.0|                 0|         158|         113|           1|        6.0|  3.0|    0.5|       0.0|         0.0|                  0.3|         9.8|                 2.5|        0.0|              6.25|          0|               3|           0|                3|                    0|                      0.0|              0|              1|                0|                 0|                           0|                            1|                0|           0|    64.5|         0.21|\n",
      "|       1| 2022-11-01 00:37:32|  2022-11-01 00:42:23|            2.0|          0.8|       1.0|                 0|         249|         158|           2|        5.5|  3.0|    0.5|       0.0|         0.0|                  0.3|         9.3|                 2.5|        0.0|              4.85|          0|               3|           0|                3|                    0|                      0.0|              0|              0|                0|                 0|                           0|                            0|                0|           0|    64.5|         0.21|\n",
      "|       1| 2022-11-01 00:48:53|  2022-11-01 01:02:00|            2.0|          2.5|       1.0|                 0|         158|         230|           1|       11.0|  3.0|    0.5|      2.95|         0.0|                  0.3|       17.75|                 2.5|        0.0|13.116666666666667|          0|               3|           1|                3|                    0|                      0.0|              0|              1|                0|                 0|                           0|                            1|                0|           0|    64.5|         0.21|\n",
      "|       2| 2022-11-01 00:13:50|  2022-11-01 00:23:06|            1.0|         2.04|       1.0|                 0|         161|         137|           2|        9.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        12.8|                 2.5|        0.0| 9.266666666666667|          0|               3|           0|                3|                    0|                      0.0|              0|              1|                0|                 0|                           1|                            0|                0|           0|    64.5|         0.21|\n",
      "|       2| 2022-11-01 00:07:31|  2022-11-01 00:21:54|            1.0|         6.63|       1.0|                 0|         138|          74|           1|       20.0|  0.5|    0.5|      5.57|        6.55|                  0.3|       34.67|                 0.0|       1.25|14.383333333333333|          0|               3|           0|                3|                    0|                      0.0|              1|              0|                1|                 0|                           0|                            0|                0|           0|    64.5|         0.21|\n",
      "|       2| 2022-11-01 00:47:57|  2022-11-01 00:50:57|            2.0|         0.53|       1.0|                 0|          48|         161|           2|        4.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         7.8|                 2.5|        0.0|               3.0|          0|               3|           0|                3|                    0|                      0.0|              0|              1|                0|                 0|                           0|                            1|                0|           0|    64.5|         0.21|\n",
      "|       2| 2022-11-01 00:37:05|  2022-11-01 00:43:18|            1.0|         1.06|       1.0|                 0|          68|         170|           2|        6.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         9.8|                 2.5|        0.0| 6.216666666666667|          0|               3|           0|                3|                    0|                      0.0|              0|              0|                0|                 0|                           0|                            0|                0|           0|    64.5|         0.21|\n",
      "|       1| 2022-11-01 00:14:05|  2022-11-01 00:23:33|            1.0|          1.5|       1.0|                 0|         164|         107|           1|        8.0|  3.0|    0.5|      2.95|         0.0|                  0.3|       14.75|                 2.5|        0.0| 9.466666666666667|          0|               3|           0|                3|                    0|                      0.0|              0|              1|                0|                 0|                           1|                            0|                0|           0|    64.5|         0.21|\n",
      "|       1| 2022-11-01 00:09:10|  2022-11-01 00:24:34|            3.0|          2.4|       1.0|                 0|         158|         148|           1|       12.0|  3.0|    0.5|      3.15|         0.0|                  0.3|       18.95|                 2.5|        0.0|              15.4|          0|               3|           0|                3|                    0|                      0.0|              0|              0|                0|                 0|                           0|                            0|                0|           0|    64.5|         0.21|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+-----------+----------------+------------+-----------------+---------------------+-------------------------+---------------+---------------+-----------------+------------------+----------------------------+-----------------------------+-----------------+------------+--------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Add a date column to the taxi data based on the pickup datetime\n",
    "sdf_with_date = sdf.withColumn('pickup_date', to_date(col('tpep_pickup_datetime')))\n",
    "df_weather = df_weather.withColumn('weather_date', to_date(col('Date')))\n",
    "\n",
    "# Join the taxi data with the weather data on the date columns\n",
    "sdf_with_temps = sdf_with_date.join(\n",
    "    df_weather.select(\n",
    "        'weather_date',\n",
    "        'Average Temperature',\n",
    "        'Precipitation'\n",
    "    ),\n",
    "    sdf_with_date['pickup_date'] == df_weather['weather_date'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Rename columns for clarity\n",
    "sdf_with_temps = sdf_with_temps.withColumnRenamed('Average Temperature', 'avg_temp') \\\n",
    "                               .withColumnRenamed('Precipitation', 'precipitation')\n",
    "\n",
    "# Drop unnecessary columns after the join\n",
    "sdf_with_temps = sdf_with_temps.drop('weather_date', 'pickup_date')\n",
    "sdf_with_temps.limit(10).show()  # Display the first 10 rows of the merged DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: 5016231 rows, 36 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "shape(sdf_with_temps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rationale for Integrating Average Temperature and Precipitation\n",
    "\n",
    "In the context of analyzing taxi trip data, `Average Temperature` and `Precipitation` are key weather factors that could significantly influence travel patterns and demand. \n",
    "\n",
    "- **Average Temperature**: Temperature impacts human behavior, potentially affecting the number of trips taken, trip distances, and passenger counts. For instance, extreme temperatures may discourage outdoor activities, reducing taxi usage.\n",
    "\n",
    "- **Precipitation**: Rain or snow directly influences transportation by potentially increasing demand for taxis (as people avoid walking or driving in bad weather) and by affecting traffic conditions, which can alter trip duration.\n",
    "\n",
    "By focusing on these two variables, we can better understand how weather conditions correlate with taxi trip characteristics, helping to refine predictive models or identify trends in transportation behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate Records\n",
    "\n",
    "### Identifying Duplicate Records\n",
    "\n",
    "Identify potential duplicate records by grouping the DataFrame by key columns (`fare_amount`, `PULocationID`, `DOLocationID`, `trip_distance`) and counting occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to check for potential duplicates\n",
    "columns_to_check = [\"fare_amount\", \"PULocationID\", \"DOLocationID\", \"trip_distance\"]\n",
    "\n",
    "# Group by the selected columns and count occurrences to identify duplicates\n",
    "duplicates_specific_columns = sdf_with_temps.groupBy(columns_to_check).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Out Frequent Duplicates\n",
    "\n",
    "Filter the DataFrame to keep only records where duplicates occur more than 15 times. This helps in identifying and removing highly repetitive records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out records where duplicates occur more than 15 times\n",
    "duplicates_sdf = duplicates_specific_columns.filter(col(\"count\") > 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: 5016231 rows, 36 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "shape(sdf_with_temps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates from the DataFrame\n",
    "\n",
    "Join the filtered duplicate records back to the main DataFrame and remove them to clean the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the main DataFrame to remove the identified duplicates\n",
    "sdf_filtered = sdf_with_temps.join(duplicates_sdf, on=columns_to_check, how='inner')\n",
    "sdf_filtered = sdf_filtered.drop(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the original columns, excluding the 'count' column\n",
    "sdf_filtered = sdf_filtered.select(sdf_with_temps.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Cleaned DataFrame Without Duplicates\n",
    "\n",
    "Subtract the filtered DataFrame from the original DataFrame to get the final cleaned DataFrame without duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 12:37:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:37:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:38:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:39:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:39:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:39:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:39:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:39:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:39:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:39:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:39:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:39:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:39:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:39:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:39:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:39:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:39:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:39:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:39:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:39:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:39:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:39:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:39:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: 4393759 rows, 36 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf_without_duplicates = sdf_with_temps.subtract(sdf_filtered)\n",
    "shape(sdf_without_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Identifying Duplicates**:\n",
    "   - **Columns Chosen**: The columns `fare_amount`, `PULocationID`, `DOLocationID`, and `trip_distance` are key identifiers for potential duplicates because they capture essential details of each trip that should typically be unique.\n",
    "   - **Grouping and Counting**: By grouping the data on these columns, we can identify cases where multiple records share identical values across these attributes, suggesting potential duplication.\n",
    "\n",
    "2. **Filtering Out Frequent Duplicates**:\n",
    "   - **Threshold Selection**: The decision to filter out records that occur more than 15 times is based on the assumption that such high-frequency duplicates are likely errors or artifacts of data processing issues. This threshold helps in focusing on genuinely redundant records without discarding too much data.\n",
    "\n",
    "3. **Removing Duplicates**:\n",
    "   - **Joining and Filtering**: By joining the filtered duplicates back to the original DataFrame and then removing them, we ensure that only those records identified as repetitive are excluded, thereby cleaning the dataset.\n",
    "   - **Final Subtraction**: The final subtraction step ensures that the cleaned DataFrame, `sdf_without_duplicates`, contains only unique records, free from the identified duplicates.\n",
    "\n",
    "=> This approach balances the need to maintain data integrity with the importance of removing noise caused by duplicate records, leading to a more accurate and reliable dataset for analysis.\n",
    "\n",
    "After cleaning, the shape of the Dataframe is 4393759 rows, 36 columns after removing duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Final Cleaned DataFrame\n",
    "\n",
    "Save the final cleaned DataFrame, which now includes integrated weather data and has duplicates removed, to a new Parquet file for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 12:40:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:17 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:40:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:41:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:41:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:41:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:41:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:41:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:41:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:41:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:41:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:41:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:41:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:41:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:41:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:41:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:41:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:41:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:41:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:41:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:41:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:41:26 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:41:26 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:41:26 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/08/25 12:41:26 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the cleaned DataFrame to a new Parquet file\n",
    "sdf_without_duplicates.write.parquet('/Users/jennymai/Desktop/data_sci/mast_project1/data/curated3', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'tpep_pickup_datetime',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'RatecodeID',\n",
       " 'store_and_fwd_flag',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'congestion_surcharge',\n",
       " 'airport_fee',\n",
       " 'trip_duration_mins',\n",
       " 'pickup_hour',\n",
       " 'pickup_dayofweek',\n",
       " 'dropoff_hour',\n",
       " 'dropoff_dayofweek',\n",
       " 'days_since_2022_11_01',\n",
       " 'distance_time_interaction',\n",
       " 'is_airport_trip',\n",
       " 'is_tourist_trip',\n",
       " 'pickup_at_airport',\n",
       " 'dropoff_at_airport',\n",
       " 'pickup_at_tourist_attraction',\n",
       " 'dropoff_at_tourist_attraction',\n",
       " 'is_holiday_season',\n",
       " 'is_event_day',\n",
       " 'avg_temp',\n",
       " 'precipitation']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_without_duplicates.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session to release resources\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
